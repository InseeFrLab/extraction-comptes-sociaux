{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "858827da-f523-4321-85c4-a9a8d2266a47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np \n",
    "import torch \n",
    "from PIL import Image \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2efcc2-6107-4db2-942a-dd321c336802",
   "metadata": {},
   "source": [
    "# Définition du dataset pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc36d6dc-b63d-4bc7-9afe-19f1500adc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PennFudanDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms):\n",
    "        self.root = root \n",
    "        self.transforms = transforms \n",
    "        \n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"PedMasks\", self.mask[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        mask = Image.open(mask_path)\n",
    "        \n",
    "        mask = np.array(mask) \n",
    "        obj_ids = np.unique(mask)\n",
    "        \n",
    "        obj_ids = np.unique(mask)\n",
    "        obj_ids = obj_ids[1:]\n",
    "        \n",
    "        mask = mask == obj_ids[:, None, None]\n",
    "        \n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        \n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "            \n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "        \n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        \n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target['boxes'] = boxes \n",
    "        target[\"labels\"] = labels \n",
    "        taget[\"mask\"] = masks \n",
    "        target[\"image_id\"] = image_id \n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "        \n",
    "        return img, target \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d86d612-ba24-491e-8469-d43f82a5d456",
   "metadata": {},
   "source": [
    "# Définition du modèle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6a40f2e-2440-4e33-9f49-a44afefe0b95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /home/onyxia/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9705da9ab0e340f498ee048a10a2a540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/160M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torchvision \n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "num_classes = 2 \n",
    "\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95fd62f5-1dd0-44b6-9284-5d9ef09c327f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Implements Faster R-CNN.\n",
      "\n",
      "    The input to the model is expected to be a list of tensors, each of shape [C, H, W], one for each\n",
      "    image, and should be in 0-1 range. Different images can have different sizes.\n",
      "\n",
      "    The behavior of the model changes depending if it is in training or evaluation mode.\n",
      "\n",
      "    During training, the model expects both the input tensors, as well as a targets (list of dictionary),\n",
      "    containing:\n",
      "        - boxes (``FloatTensor[N, 4]``): the ground-truth boxes in ``[x1, y1, x2, y2]`` format, with\n",
      "          ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``.\n",
      "        - labels (Int64Tensor[N]): the class label for each ground-truth box\n",
      "\n",
      "    The model returns a Dict[Tensor] during training, containing the classification and regression\n",
      "    losses for both the RPN and the R-CNN.\n",
      "\n",
      "    During inference, the model requires only the input tensors, and returns the post-processed\n",
      "    predictions as a List[Dict[Tensor]], one for each input image. The fields of the Dict are as\n",
      "    follows:\n",
      "        - boxes (``FloatTensor[N, 4]``): the predicted boxes in ``[x1, y1, x2, y2]`` format, with\n",
      "          ``0 <= x1 < x2 <= W`` and ``0 <= y1 < y2 <= H``.\n",
      "        - labels (Int64Tensor[N]): the predicted labels for each image\n",
      "        - scores (Tensor[N]): the scores or each prediction\n",
      "\n",
      "    Args:\n",
      "        backbone (nn.Module): the network used to compute the features for the model.\n",
      "            It should contain a out_channels attribute, which indicates the number of output\n",
      "            channels that each feature map has (and it should be the same for all feature maps).\n",
      "            The backbone should return a single Tensor or and OrderedDict[Tensor].\n",
      "        num_classes (int): number of output classes of the model (including the background).\n",
      "            If box_predictor is specified, num_classes should be None.\n",
      "        min_size (int): minimum size of the image to be rescaled before feeding it to the backbone\n",
      "        max_size (int): maximum size of the image to be rescaled before feeding it to the backbone\n",
      "        image_mean (Tuple[float, float, float]): mean values used for input normalization.\n",
      "            They are generally the mean values of the dataset on which the backbone has been trained\n",
      "            on\n",
      "        image_std (Tuple[float, float, float]): std values used for input normalization.\n",
      "            They are generally the std values of the dataset on which the backbone has been trained on\n",
      "        rpn_anchor_generator (AnchorGenerator): module that generates the anchors for a set of feature\n",
      "            maps.\n",
      "        rpn_head (nn.Module): module that computes the objectness and regression deltas from the RPN\n",
      "        rpn_pre_nms_top_n_train (int): number of proposals to keep before applying NMS during training\n",
      "        rpn_pre_nms_top_n_test (int): number of proposals to keep before applying NMS during testing\n",
      "        rpn_post_nms_top_n_train (int): number of proposals to keep after applying NMS during training\n",
      "        rpn_post_nms_top_n_test (int): number of proposals to keep after applying NMS during testing\n",
      "        rpn_nms_thresh (float): NMS threshold used for postprocessing the RPN proposals\n",
      "        rpn_fg_iou_thresh (float): minimum IoU between the anchor and the GT box so that they can be\n",
      "            considered as positive during training of the RPN.\n",
      "        rpn_bg_iou_thresh (float): maximum IoU between the anchor and the GT box so that they can be\n",
      "            considered as negative during training of the RPN.\n",
      "        rpn_batch_size_per_image (int): number of anchors that are sampled during training of the RPN\n",
      "            for computing the loss\n",
      "        rpn_positive_fraction (float): proportion of positive anchors in a mini-batch during training\n",
      "            of the RPN\n",
      "        rpn_score_thresh (float): during inference, only return proposals with a classification score\n",
      "            greater than rpn_score_thresh\n",
      "        box_roi_pool (MultiScaleRoIAlign): the module which crops and resizes the feature maps in\n",
      "            the locations indicated by the bounding boxes\n",
      "        box_head (nn.Module): module that takes the cropped feature maps as input\n",
      "        box_predictor (nn.Module): module that takes the output of box_head and returns the\n",
      "            classification logits and box regression deltas.\n",
      "        box_score_thresh (float): during inference, only return proposals with a classification score\n",
      "            greater than box_score_thresh\n",
      "        box_nms_thresh (float): NMS threshold for the prediction head. Used during inference\n",
      "        box_detections_per_img (int): maximum number of detections per image, for all classes.\n",
      "        box_fg_iou_thresh (float): minimum IoU between the proposals and the GT box so that they can be\n",
      "            considered as positive during training of the classification head\n",
      "        box_bg_iou_thresh (float): maximum IoU between the proposals and the GT box so that they can be\n",
      "            considered as negative during training of the classification head\n",
      "        box_batch_size_per_image (int): number of proposals that are sampled during training of the\n",
      "            classification head\n",
      "        box_positive_fraction (float): proportion of positive proposals in a mini-batch during training\n",
      "            of the classification head\n",
      "        bbox_reg_weights (Tuple[float, float, float, float]): weights for the encoding/decoding of the\n",
      "            bounding boxes\n",
      "\n",
      "    Example::\n",
      "\n",
      "        >>> import torch\n",
      "        >>> import torchvision\n",
      "        >>> from torchvision.models.detection import FasterRCNN\n",
      "        >>> from torchvision.models.detection.rpn import AnchorGenerator\n",
      "        >>> # load a pre-trained model for classification and return\n",
      "        >>> # only the features\n",
      "        >>> backbone = torchvision.models.mobilenet_v2(weights=MobileNet_V2_Weights.DEFAULT).features\n",
      "        >>> # FasterRCNN needs to know the number of\n",
      "        >>> # output channels in a backbone. For mobilenet_v2, it's 1280\n",
      "        >>> # so we need to add it here\n",
      "        >>> backbone.out_channels = 1280\n",
      "        >>>\n",
      "        >>> # let's make the RPN generate 5 x 3 anchors per spatial\n",
      "        >>> # location, with 5 different sizes and 3 different aspect\n",
      "        >>> # ratios. We have a Tuple[Tuple[int]] because each feature\n",
      "        >>> # map could potentially have different sizes and\n",
      "        >>> # aspect ratios\n",
      "        >>> anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
      "        >>>                                    aspect_ratios=((0.5, 1.0, 2.0),))\n",
      "        >>>\n",
      "        >>> # let's define what are the feature maps that we will\n",
      "        >>> # use to perform the region of interest cropping, as well as\n",
      "        >>> # the size of the crop after rescaling.\n",
      "        >>> # if your backbone returns a Tensor, featmap_names is expected to\n",
      "        >>> # be ['0']. More generally, the backbone should return an\n",
      "        >>> # OrderedDict[Tensor], and in featmap_names you can choose which\n",
      "        >>> # feature maps to use.\n",
      "        >>> roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
      "        >>>                                                 output_size=7,\n",
      "        >>>                                                 sampling_ratio=2)\n",
      "        >>>\n",
      "        >>> # put the pieces together inside a FasterRCNN model\n",
      "        >>> model = FasterRCNN(backbone,\n",
      "        >>>                    num_classes=2,\n",
      "        >>>                    rpn_anchor_generator=anchor_generator,\n",
      "        >>>                    box_roi_pool=roi_pooler)\n",
      "        >>> model.eval()\n",
      "        >>> x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
      "        >>> predictions = model(x)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from torchvision.models.detection.rpn import AnchorGenerator \n",
    "from torchvision.models.detection import FasterRCNN\n",
    "backbone = torchvision.models.mobilenet_v2(weights=\"DEFAULT\").features \n",
    "\n",
    "backbone.out_channels = 1280 \n",
    "\n",
    "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                   aspect_ratios=((0.5, 1.0,2.0),))\n",
    "\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
    "                                                output_size=7,\n",
    "                                                sampling_ratio=2)\n",
    "\n",
    "model = FasterRCNN(backbone, \n",
    "                   num_classes=2,\n",
    "                   rpn_anchor_generator=anchor_generator,\n",
    "                   box_roi_pool=roi_pooler)\n",
    "print(model.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13cc55f2-b1cb-46a2-bb7f-648fb50a42c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision \n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor \n",
    "\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    model = torchvision.models.detections.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "    \n",
    "    in_features = model.roi.head.box_predictor.cls_score.in_features\n",
    "    \n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    in_features_mask = model.roi_heads_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    \n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, \n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ad9d3f6-1692-4a2d-9d13-80908e65271a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'escape' from 'cgi' (/opt/mamba/lib/python3.10/cgi.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mT\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_transforms\u001b[39m(train):\n\u001b[1;32m      4\u001b[0m     transforms \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/transforms/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msafe_html\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m safe_html, bodyfinder\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# modules = [\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#     'st',             # zopish\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#     'rest',           # docutils\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#     'html_to_web_intelligent_plain_text',\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#     ]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/mamba/lib/python3.10/site-packages/transforms/safe_html.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msgmllib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SGMLParser, SGMLParseError\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcgi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m escape\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m safeToInt\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msafe_html_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'escape' from 'cgi' (/opt/mamba/lib/python3.10/cgi.py)"
     ]
    }
   ],
   "source": [
    "import transforms as T\n",
    "\n",
    "def get_transforms(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.PILToTensor())\n",
    "    transforms.append(T.ConvertImageDtype(torch.flat))\n",
    "    if train:\n",
    "        tranforms.append(T.RandomHonrizontalFlip(0.5))\n",
    "        \n",
    "    return T.Compose(transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1648ad-2827-42c6-b5d5-30da098799a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
